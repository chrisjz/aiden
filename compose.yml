version: '3.9'

x-base_service: &base_service
  pull_policy: always
  tty: true
  restart: unless-stopped

services:
  auditory-ambient-api:
    <<: *base_service
    profiles: ["auditory-ambient-gpu"]
    container_name: auditory-ambient-api
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    image: chrisjz/yamnet:latest
    ports:
      - "${AUDITORY_AMBIENT_API_PORT:-9001}:8000"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

  auditory-ambient-api-cpu:
    <<: *base_service
    profiles: ["auditory-ambient-cpu"]
    container_name: auditory-ambient-api
    deploy: {}
    image: chrisjz/yamnet:latest
    ports:
      - "${AUDITORY_AMBIENT_API_PORT:-9001}:8000"

  auditory-language-api:
    <<: *base_service
    profiles: ["auditory-language-gpu"]
    container_name: auditory-language-api
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    image: onerahmet/openai-whisper-asr-webservice:latest-gpu
    ports:
      - "${AUDITORY_LANGUAGE_API_PORT:-9000}:9000"
    environment:
      - ASR_MODEL=${AUDITORY_LANGUAGE_MODEL:-tiny}
      - ASR_ENGINE=${AUDITORY_LANGUAGE_API_ENGINE:-faster_whisper}

  auditory-language-api-cpu:
    <<: *base_service
    profiles: ["auditory-language-cpu"]
    container_name: auditory-language-api
    deploy: {}
    image: onerahmet/openai-whisper-asr-webservice:latest
    ports:
      - "${AUDITORY_LANGUAGE_API_PORT:-9000}:9000"
    environment:
      - ASR_MODEL=${AUDITORY_LANGUAGE_MODEL:-tiny}
      - ASR_ENGINE=${AUDITORY_LANGUAGE_API_ENGINE:-faster_whisper}

  brain-api:
    build: .
    ports:
      - "${BRAIN_API_PORT:-8000}:8000"
    environment:
      - CHROMA_HOST=chroma
      - COGNITIVE_API_HOST=cognitive-api
      - COGNITIVE_API_PORT=11434
      - COGNITIVE_API_PROTOCOL=${COGNITIVE_API_PROTOCOL:-http}
      - COGNITIVE_MODEL=${COGNITIVE_MODEL:-mistral}
      - REDIS_HOST=redis
      - VISION_API_HOST=vision-api
      - VISION_API_PORT=11434
      - VISION_API_PROTOCOL=${VISION_API_PROTOCOL:-http}
      - VISION_MODEL=${VISION_MODEL:-bakllava}
    depends_on:
      - chroma
      - redis

  chroma:
    image: chromadb/chroma:latest
    environment:
      - IS_PERSISTENT=TRUE
    volumes:
      - ./data/memory:/chroma/chroma/
    ports:
      - 8432:8000

  cognitive-api:
    <<: *base_service
    profiles: ["cognitive-gpu"]
    container_name: cognitive-api
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    image: ollama/ollama
    ports:
      - "${COGNITIVE_API_PORT:-11434}:11434"
    environment:
      - COGNITIVE_MODEL=${COGNITIVE_MODEL:-mistral}
    entrypoint: ["sh", "-c", "ollama start & sleep 5 && ollama run ${COGNITIVE_MODEL}"]

  cognitive-api-cpu:
    <<: *base_service
    profiles: ["cognitive-cpu"]
    container_name: cognitive-api
    deploy: {}
    image: ollama/ollama
    ports:
      - "${COGNITIVE_API_PORT:-11434}:11434"
    environment:
      - COGNITIVE_MODEL=${COGNITIVE_MODEL:-mistral}
    entrypoint: ["sh", "-c", "ollama start & sleep 5 && ollama run ${COGNITIVE_MODEL}"]

  redis:
    image: redis:latest
    ports:
      - "${REDIS_PORT:-6379}:6379"
    command: redis-server
    environment:
      - ALLOW_EMPTY_PASSWORD=yes

  vision-api:
    <<: *base_service
    profiles: ["vision-gpu"]
    container_name: vision-api
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    image: ollama/ollama
    ports:
      - "${VISION_API_PORT:-11435}:11434"
    environment:
      - VISION_API_HOST=${VISION_API_HOST:-localhost}
      - VISION_API_PROTOCOL=${VISION_API_PROTOCOL:-http}
      - VISION_MODEL=${VISION_MODEL:-bakllava}
    entrypoint: ["sh", "-c", "ollama start & sleep 5 && ollama run ${VISION_MODEL}"]

  vision-api-cpu:
    <<: *base_service
    profiles: ["vision-cpu"]
    container_name: vision-api
    deploy: {}
    image: ollama/ollama
    ports:
      - "${VISION_API_PORT:-11435}:11434"
    environment:
      - VISION_MODEL=${VISION_MODEL:-bakllava}
    entrypoint: ["sh", "-c", "ollama start & sleep 5 && ollama run ${VISION_MODEL}"]

  vocal-api:
    <<: *base_service
    profiles: ["vocal-gpu"]
    container_name: vocal-api
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    image: ghcr.io/coqui-ai/tts
    ports:
      - "${VOCAL_API_PORT:-5002}:5002"
    entrypoint: ["python3"]
    command: ["TTS/server/server.py", "--model_name", "${VOCAL_MODEL:-tts_models/en/ljspeech/tacotron2-DDC}", "--use_cuda", "true"]
    environment:
      - MODEL=${VOCAL_MODEL:-tts_models/en/ljspeech/tacotron2-DCA}
      - VOCODER=${VOCAL_API_VOCODER:-vocoder_models/en/ljspeech/multiband-melgan}

  vocal-api-cpu:
    <<: *base_service
    profiles: ["vocal-cpu"]
    container_name: vocal-api
    deploy: {}
    image: ghcr.io/coqui-ai/tts-cpu
    ports:
      - "${VOCAL_API_PORT:-5002}:5002"
    entrypoint: ["python3"]
    command: ["TTS/server/server.py", "--model_name", "${VOCAL_MODEL:-tts_models/en/ljspeech/tacotron2-DDC}"]
    environment:
      - MODEL=${VOCAL_MODEL:-tts_models/en/ljspeech/tacotron2-DCA}
      - VOCODER=${VOCAL_API_VOCODER:-vocoder_models/en/ljspeech/multiband-melgan}
