version: '3.9'

x-base_service: &base_service
  pull_policy: always
  tty: true
  restart: always

services:
  auditory-api:
    <<: *base_service
    profiles: ["auditory-gpu"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    image: onerahmet/openai-whisper-asr-webservice:latest-gpu
    ports:
      - "${AUDITORY_API_PORT:-9000}:9000"
    environment:
      - ASR_MODEL=${AUDITORY_API_MODEL:-tiny}
      - ASR_ENGINE=${AUDITORY_API_ENGINE:-faster_whisper}

  auditory-api-cpu:
    <<: *base_service
    profiles: ["auditory-cpu"]
    deploy: {}
    image: onerahmet/openai-whisper-asr-webservice:latest
    ports:
      - "${AUDITORY_API_PORT:-9000}:9000"
    environment:
      - ASR_MODEL=${AUDITORY_API_MODEL:-tiny}
      - ASR_ENGINE=${AUDITORY_API_ENGINE:-faster_whisper}

  cognitive-api:
    <<: *base_service
    profiles: ["cognitive-gpu"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    image: ollama/ollama
    ports:
      - "${COGNITIVE_API_PORT:-11434}:11434"
    environment:
      - COGNITIVE_MODEL=${COGNITIVE_MODEL:-mistral}
    entrypoint: ["sh", "-c", "ollama start & sleep 5 && ollama run ${COGNITIVE_MODEL}"]

  cognitive-api-cpu:
    <<: *base_service
    profiles: ["cognitive-cpu"]
    deploy: {}
    image: ollama/ollama
    ports:
      - "${COGNITIVE_API_PORT:-11434}:11434"
    environment:
      - COGNITIVE_MODEL=${COGNITIVE_MODEL:-mistral}
    entrypoint: ["sh", "-c", "ollama start & sleep 5 && ollama run ${COGNITIVE_MODEL}"]

  vision-api:
    <<: *base_service
    profiles: ["vision-gpu"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    image: ollama/ollama
    ports:
      - "${VISION_API_PORT:-11435}:11434"
    environment:
      - VISION_API_HOST=${VISION_API_HOST:-localhost}
      - VISION_API_PROTOCOL=${VISION_API_PROTOCOL:-http}
      - VISION_MODEL=${VISION_MODEL:-llava}
    entrypoint: ["sh", "-c", "ollama start & sleep 5 && ollama run ${VISION_MODEL}"]

  vision-api-cpu:
    <<: *base_service
    profiles: ["vision-cpu"]
    deploy: {}
    image: ollama/ollama
    ports:
      - "${VISION_API_PORT:-11435}:11434"
    environment:
      - VISION_MODEL=${VISION_MODEL:-llava}
    entrypoint: ["sh", "-c", "ollama start & sleep 5 && ollama run ${VISION_MODEL}"]

  vocal-api:
    build:
      context: .
      dockerfile: ./compose/vocal/Dockerfile
    ports:
      - "${VOCAL_API_PORT}:80"
